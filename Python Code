#!/usr/bin/env python
import cv2 as cv
import numpy as np
import roslib
import sys
import rospy
from sensor_msgs.msg import Image
from cv_bridge import CvBridge, CvBridgeError
import tf2_ros
import geometry_msgs.msg
import tf_conversions
import tf
import copy
import moveit_commander
import moveit_msgs.msg
import actionlib
import math
from time import sleep
from geometry_msgs.msg import Twist
from sensor_msgs.msg import LaserScan
from nav_msgs.msg import Odometry
from tf.transformations import euler_from_quaternion

#Global variables
num_tomatoes = 0
z = 0
counter = 0
co_ordinates = []
flag = 0
Pose = []
regions= {}
trough = 0
flag = 0
state = 0
left = 0
uturn_1= 0
left_2 = 0
park11 = 0
park112 = 0
markerID = 0	

#This program is the only program for Task 5 ( both navigation and perception)

class Ur5Moveit:

	# Constructor
	def __init__(self):


		self._planning_group = "ur5_1_planning_group"
		self._commander = moveit_commander.roscpp_initialize(sys.argv)
		self._robot = moveit_commander.RobotCommander()
		self._scene = moveit_commander.PlanningSceneInterface()
		self._group = moveit_commander.MoveGroupCommander(self._planning_group)
		self._display_trajectory_publisher = rospy.Publisher(
			'/move_group/display_planned_path', moveit_msgs.msg.DisplayTrajectory, queue_size=1)

		self._exectute_trajectory_client = actionlib.SimpleActionClient(
			'execute_trajectory', moveit_msgs.msg.ExecuteTrajectoryAction)
		self._exectute_trajectory_client.wait_for_server()

		self._planning_frame = self._group.get_planning_frame()
		self._eef_link = self._group.get_end_effector_link()
		self._group_names = self._robot.get_group_names()
		self._box_name = ''

		# Current State of the Robot is needed to add box to planning scene
		self._curr_state = self._robot.get_current_state()

	def set_joint_angles(self, arg_list_joint_angles):

		list_joint_values = self._group.get_current_joint_values()

		self._group.set_joint_value_target(arg_list_joint_angles)
		self._group.plan()
		flag_plan = self._group.go(wait=True)

		list_joint_values = self._group.get_current_joint_values()

		pose_values = self._group.get_current_pose().pose

		return flag_plan
	
	def go_to_pose(self, arg_pose):
		pose_values = self._group.get_current_pose().pose
		
		self._group.set_pose_target(arg_pose)
		flag_plan = self._group.go(wait=True)  # wait=False for Async Move

		pose_values = self._group.get_current_pose().pose

		list_joint_values = self._group.get_current_joint_values()

		return flag_plan

	# Destructor
	def __del__(self):
		moveit_commander.roscpp_shutdown()


class gripperMoveit:

	# Constructor
	def __init__(self):
		self._planning_group = "gripper"
		self._commander = moveit_commander.roscpp_initialize(sys.argv)
		self._robot = moveit_commander.RobotCommander()
		self._scene = moveit_commander.PlanningSceneInterface()
		self._group = moveit_commander.MoveGroupCommander(self._planning_group)
		self._display_trajectory_publisher = rospy.Publisher(
			'/move_group/display_planned_path', moveit_msgs.msg.DisplayTrajectory, queue_size=1)

		self._exectute_trajectory_client = actionlib.SimpleActionClient(
			'execute_trajectory', moveit_msgs.msg.ExecuteTrajectoryAction)
		self._exectute_trajectory_client.wait_for_server()

		self._planning_frame = self._group.get_planning_frame()
		self._eef_link = self._group.get_end_effector_link()
		self._group_names = self._robot.get_group_names()

	def set_joint_angles(self, arg_list_joint_angles):

		list_joint_values = self._group.get_current_joint_values()

		self._group.set_joint_value_target(arg_list_joint_angles)
		self._group.plan()
		flag_plan = self._group.go(wait=True)

		list_joint_values = self._group.get_current_joint_values()

		pose_values = self._group.get_current_pose().pose

		return flag_plan



def givedelay():
	time = rospy.Time.now()
	while rospy.Time.now() < time + rospy.Duration.from_sec(3):
	   pass

def odom_callback(data):
	"""Call back function of odometry"""
	global Pose
	x = data.pose.pose.orientation.x
	y = data.pose.pose.orientation.y
	z = data.pose.pose.orientation.z
	w = data.pose.pose.orientation.w
	Pose = [data.pose.pose.position.x, data.pose.pose.position.y, euler_from_quaternion([x,y,z,w])[2]]

def laser_callback(msg):
	""" Callback function of laser """	
	global regions
	regions = {'left': min(min(msg.ranges[700:720]), 10) ,'right': min(min(msg.ranges[0:20]), 10) ,'front': min(min(msg.ranges[350:370]), 10)}

def move(ang_vel , linear_vel):
	"""function to publish velocity to /cmd_vel """
	vel = Twist()
	pub = rospy.Publisher('/cmd_vel', Twist, queue_size=10)
	rate = rospy.Rate(10)
	vel.angular.z = ang_vel
	vel.linear.x = linear_vel
	pub.publish(vel)
	rate.sleep()


def turn_rightlane():
	"""function to traverse through the first column"""
	global park112
	
	if (park112 == 0):
		
		while Pose[2]<1.57:
			move(0.7,0)
		move(0,0)
		
	park112 = 1	


def turn_right_to_center():
	global left
	global ur5
	if(left == 0):
		all_zero = [0,0.52,0,0,0,0]
		ur5.set_joint_angles(all_zero)
		while Pose[1] < 8.5:
			move(0,0.7)
		while Pose[2]>0:
			move(0.7,0)
		while regions['front'] >= 3.25:
			move(0,0.5)
		while(Pose[2]<-1.57):
			move(0.5,0)
		move(0,0)
		first_angles = [1.57,-0.61,-1,1.585,0,0]
		ur5.set_joint_angles(first_angles)
	left = 1
	
def turn_center_to_left():
	"""Turn left"""
	global left_2
	global ur5
	if (left_2 == 0):
		all_zero = [0,0.5,0,0,0,0]
		ur5.set_joint_angles(all_zero)
		while Pose[1] < 8.5:
			move(0,0.5)
		while Pose[2]>0:
			move(0.5,0)
		while regions['front'] >= 1.7:
			move(0,0.5)
		while(Pose[2]<-1.57):
			move(0.5,0)
		move(0,0)
		first_angles = [1.57,-0.61,-1,1.585,0,0]
		ur5.set_joint_angles(first_angles)
	left_2 = 1
		

def Gain(lErr):
	if (lErr < 0):
		if (lErr > 0.2):
			return 1.0
		elif (lErr > 0.1):
			return 0.5
		else:
			return 0.25
	else:
		return 1.0
	

def nav_right1019():
	global trough
	global state
	while True:
		if (state == 4):
			if (Pose[1] < -0.05 + (0.8*(trough - 20))):
				if regions['left'] > 0.65 and regions['left'] < 1.5:
					move(0.2,0.4)
				elif regions['left'] < 0.65:
					move(-0.2,0.4)
				else:
					move(0,0.5)
			else:
				move(0,0)
				reorient()
				trough = trough + 1
				state = 1
		else:
			break

def nav_center1910():
	global trough
	global state
	while True:
		if (state == 5):
			if (Pose[1] > 7.2 - (0.8*(trough-30))):
				if(trough < 37):
					if regions['left'] > 0.5 and regions['left'] < 1.1:
						move(0.2,0.4)
					elif regions['left'] < 0.5:
						move(-0.2,0.4)
					else:
						move(0,0.5)
				else:
					if ((regions['right'] > 0.5) and (regions['right']<1.1)):
						move(-0.2,0.5)
					if (regions['right'] < 0.5):
						move(0.2,0.5)
					else:
						move(0,0.5)
			else:
				move(0,0)
				reorient_2nd_lane()
				trough = trough + 1
				state = 1
		else:
			break
			
def nav_left90():
	global trough
	global state
	while True:
		if (state == 3):
			if (Pose[1] > 7.2 - (0.8*(trough-10))):
				if regions['left'] > 0.5 and regions['left'] < 1.1:
					move(0.25,0.4)
				elif regions['left'] < 0.5:
					move(-0.25,0.4)
				else:
					move(0,0.5)
			else:
				move(0,0)
				reorient_2nd_lane()
				trough = trough + 1
				state = 1
		else:
			break			

def finalpark():
	global state
	while True:
		if (state == 6):	
			while Pose[1] > -1.5:
				if regions['left'] > 0.5 and regions['left'] < 1.1:
					move(0.2,0.4)
				elif regions['left'] < 0.5:
					move(-0.2,0.4)
				else:
					move(0,0.5)
			move(0,0)
			break
		else:
			break				
			
def nav_left_to_right():
	
	global park11
	
	if (park11 == 0):

		while Pose[1] > -1.6:
			if regions['left'] > 0.5 and regions['left'] < 1.1:
				move(0.2,0.4)
			elif regions['left'] < 0.5:
				move(-0.2,0.4)
			else:
				move(0,0.5)
		move(0,0)
		while Pose[2] < 0:
			move(0.5,0)
		move(0,0)
		while regions['front'] > 1.6:
			move(0,0.7)
		move(0,0)

	park11 = 1		
			

def reorient():
	global Pose
	if Pose[2] > 1.57:
		while Pose[2] > 1.57:
			move(-0.3,0)
		move(0,0)
	else:
		while Pose[2] < 1.57:
			move(0.3,0)
		move(0,0)

def reorient_2nd_lane():
	global Pose
	if Pose[2] > -1.57:
		while Pose[2] > -1.57:
			move(-0.3,0)
		move(0,0)
	else:
		while Pose[2] < -1.57:
			move(0.3,0)
		move(0,0)

def nav_center09():
	global trough
	global state
	while True:
		if (state == 0):
			if (Pose[1] < (0.8*(trough))):
				
				gErr = (-0.1 + 0.8*(trough) - Pose[1])
				
				gGain = Gain(gErr)
				
				if ((regions['left'] > 0.65) and (regions['left'] < 1.1)):
						move(0.15,(0.4 * gGain))
				if regions['left'] < 0.65:
					move(-0.25,(0.4 * gGain))
				else:
					move(0.15,0.5)
			else:
				move(0,0)
				reorient()
				move (0,0)
				trough = trough + 1
				state = 1
				#Wait for Marke to be visible and captured
				sleep (0.05)
		else:
			break

def aruco_callback(data_aruco):
	# Initializing variables
	focal_length = 476.70308
	center_x = 400.5
	center_y = 400.5
	aruco_dimension = 0.1
	global state
	global markerID
	try:
		bridge = CvBridge()
		frame = bridge.imgmsg_to_cv2(data_aruco, "bgr8")
		frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)
		#cv.imshow("frame", frame)
		#cv.waitKey(1)
		if(state == 1):
			# load the dictionary that was used to generate the markers
			dictionary = cv.aruco.Dictionary_get(cv.aruco.DICT_7X7_1000)
	
			# initializing the detector parameters with default values
			parameters =  cv.aruco.DetectorParameters_create()
	
			# detect the markers in the frame
			corners, ids, rejectedCandidates = cv.aruco.detectMarkers(frame, dictionary, parameters=parameters)
	
			if len(corners) > 0:
				# Flatten the ArUco IDs list
				ids = ids.flatten()
				# loop over the detected ArUCo corners
				for (markerCorner, markerID) in zip(corners, ids):
					# extract the marker corners (which are always returned
					# in top-left, top-right, bottom-right, and bottom-left
					# order)
					corners = markerCorner.reshape((4, 2))
					(topLeft, topRight, bottomRight, bottomLeft) = corners
					# convert each of the (x, y)-coordinate pairs to integers
					topRight = (int(topRight[0]), int(topRight[1]))
					bottomRight = (int(bottomRight[0]), int(bottomRight[1]))
					bottomLeft = (int(bottomLeft[0]), int(bottomLeft[1]))
					topLeft = (int(topLeft[0]), int(topLeft[1]))
	
					# draw the bounding box of the ArUCo detection
					cv.line(frame, topLeft, topRight, (0, 255, 0), 2)
					cv.line(frame, topRight, bottomRight, (0, 255, 0), 2)
					cv.line(frame, bottomRight, bottomLeft, (0, 255, 0), 2)
					cv.line(frame, bottomLeft, topLeft, (0, 255, 0), 2)
					# compute and draw the center (x, y)-coordinates of the ArUco
					# marker
					cX = int((topLeft[0] + bottomRight[0]) / 2.0)
					cY = int((topLeft[1] + bottomRight[1]) / 2.0)
					cv.circle(frame, (cX, cY), 4, (0, 0, 255), -1)
					
					pixel_width = topLeft[1] - bottomRight[1]
	
					# draw the ArUco marker ID on the frame
					cv.putText(frame, str(markerID),
						(topLeft[0], topLeft[1] - 15), cv.FONT_HERSHEY_SIMPLEX,
						0.5, (0, 255, 0), 2)
					
					'''uncomment to view aruco ID and verify the working of the code'''
					print("{} Reached".format(markerID))
	
					# obtain depth for each ArUco marker
					distance = (focal_length*aruco_dimension)/pixel_width
	
					# transforming pixel coordinates to world coordinates
					world_x = (cX - center_x)/focal_length*distance
					world_y = (cY - center_y)/focal_length*distance
					world_z = distance
	
					# broadcasting TF for each aruco marker
					br = tf2_ros.TransformBroadcaster()
					t = geometry_msgs.msg.TransformStamped()
					t.header.stamp = rospy.Time.now()
					t.header.frame_id = "sjcam_link"
					t.child_frame_id = "aruco"+str(markerID)
	
					# putting world coordinates coordinates as viewed for sjcam frame
					t.transform.translation.x = world_z
					t.transform.translation.y = -world_x
					t.transform.translation.z = world_y
					# not extracting any orientation thus orientation is (0, 0, 0)
					q = tf_conversions.transformations.quaternion_from_euler(0, 0, 0)
					t.transform.rotation.x = q[0]
					t.transform.rotation.y = q[1]
					t.transform.rotation.z = q[2]
					t.transform.rotation.w = q[3]
	
					#br.sendTransform(t)
			state = 12
			'''uncoment to view the visual of detection'''
	except CvBridgeError as e:
		print(e)



#call_back function of depth cam
def depth_callback(data_depth):
	global counter
	counter = counter + 1
	global pose_estimate
	global co_ordinates
	global z
	global trough
	global state
	global tfbuffer
	global listener		
	
	if state == 12:
		depth_values = [0]*num_tomatoes
		#lists to hold calculated XYZ co-ordinates from color and depth image
		Z_values = [0]*num_tomatoes
		Y_values = [0]*num_tomatoes
		X_values = [0]*num_tomatoes
		pose_estimate = [(0,0,0)]*num_tomatoes
		world_cordinates = [(0,0,0)]*num_tomatoes
		#specifications of camera
		focal_length = 554.387
		center_x = 320.5
		center_y = 240.5
		try:
			if (num_tomatoes != 0):
				bridge = CvBridge()
				frame_depth = bridge.imgmsg_to_cv2(data_depth, "32FC1")
				for k in range(num_tomatoes):
					#using the formula to calculate 3D cordinates from 2D image pixel cordinates
					depth_values[k] = frame_depth[co_ordinates[k][1],co_ordinates[k][0]]
					Z_values[k] = depth_values[k]
					X_values[k] = depth_values[k]*((co_ordinates[k][0] - center_x)/focal_length)
					Y_values[k] = depth_values[k]*((co_ordinates[k][1] - center_y)/focal_length)
					world_cordinates = list(zip(X_values, Y_values, Z_values))
				for g in range(len(world_cordinates)):
					if world_cordinates[g][0] != 0:
						#publishing the tf from the camera_link2 frame
						br = tf2_ros.TransformBroadcaster()
						t = geometry_msgs.msg.TransformStamped()
						t.header.stamp = rospy.Time.now()
						t.header.frame_id = "camera_link2"
						t.child_frame_id = f"obj{g}"
						
						t.transform.translation.x = world_cordinates[g][2]
						t.transform.translation.y = -world_cordinates[g][0]
						t.transform.translation.z = -world_cordinates[g][1]
						q = tf_conversions.transformations.quaternion_from_euler(0, 0, 0)
						t.transform.rotation.x = q[0]
						t.transform.rotation.y = q[1]
						t.transform.rotation.z = q[2]
						t.transform.rotation.w = q[3]
		
						#br.sendTransform(t)
						try:
							trans = tfbuffer.lookup_transform("ebot_base","camera_link2", rospy.Time())
							#trans = tfbuffer.lookup_transform("ebot_base","camera_link2", rospy.Time())
							x_tf = trans.transform.translation.x
							y_tf = trans.transform.translation.y
							z_tf = trans.transform.translation.z
							pose_estimate[g] = ((world_cordinates[g][0] + x_tf),(world_cordinates[g][2] + y_tf),(-world_cordinates[g][1] + z_tf))
							state = 2
						except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException) as e:
							print ("Fail", e)
							state = 12
			else:
				if (trough < 10):
					state = 0
				elif (trough < 20):
					state = 3
				elif (trough < 30):
					state = 4
				elif (trough < 40):
					state = 5	
				else:
					state = 6
		except CvBridgeError as e:
			print(e)
			state = 0

def nothing(x):
	pass
#callback function of color camera
def color_callback(data):
	global co_ordinates
	global num_tomatoes
	global z
	global tfbuffer
	global listener		
	
	j = 0
	focal_length = 554.387
	center_x = 320.5
	center_y = 240.5
	try:
		bridge = CvBridge()
		frame = bridge.imgmsg_to_cv2(data, "bgr8")
		hsv = cv.cvtColor(frame , cv.COLOR_BGR2HSV)
		#specifying the lower and upper hsv values of red 
		l_r = np.array([0,15,47])
		u_r = np.array([0,255,255])
		mask = cv.inRange( hsv , l_r , u_r)
		#finding the contour
		contours, hierarchy = cv.findContours(mask, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)
		num_tomatoes = len(contours)
		if(True):
			j = 0
			depth_values = [0]*len(contours)
			cx_values = [0]*len(contours)
			cy_values = [0]*len(contours)
			z = 0
			for i in contours:
				M = cv.moments(i)
				if M['m00'] != 0:
					#finding the centre of the contour
					cx = int(M['m10']/M['m00'])
					cy = int(M['m01']/M['m00'])
					#drawing a bounding box and plotting the centre of the contours
					cv.drawContours(frame, [i], -1, (255, 0, 250), 2)
					cv.circle(frame, (cx, cy), 1, (255, 255, 255), -1)
					cx_values[j] = cx
					cy_values[j] = cy
					j = j + 1
					cv.putText(frame, f"obj{z}", (cx - 20, cy - 20), cv.FONT_HERSHEY_SIMPLEX, 0.5, (250, 250, 250), 2)
					z = z + 1
		co_ordinates = list(zip(cx_values,cy_values))
		#cv.imshow("camera_image_color",frame)
		#cv.waitKey(1)
	except CvBridgeError as e:
		print(e)
    
def main_program():
	global state
	global trough
	global markerID
	global pose_estimate
	global flag    		
	global tfbuffer
	global listener	
	global ur5	
	print("Started Run!")
	rospy.init_node('Task_5', anonymous=True)
	rospy.Subscriber('/odom', Odometry, odom_callback)
	rospy.Subscriber('/ebot/laser/scan', LaserScan, laser_callback)
	givedelay()
	ur5 = Ur5Moveit()
	gripper = gripperMoveit()
	gripper_angle_close = [0.78]
	gripper_angle_open = [0]
	first_angles = [1.57,-0.61,-1,1.585,0,0]
	ur5.set_joint_angles(first_angles)
	tfbuffer = tf2_ros.Buffer()
	listener = tf2_ros.TransformListener(tfbuffer)
	rate = rospy.Rate(10.0)
	image_sub_aruco = rospy.Subscriber("/ebot/camera1/image_raw", Image, aruco_callback)
	image_sub = rospy.Subscriber("/camera/color/image_raw2", Image, color_callback)
	# subscribing to /camera/color/image_raw2 topic which is the image frame of camera in gripper
	image_sub_depth = rospy.Subscriber("/camera/depth/image_raw2", Image, depth_callback)
	while True:
		if (state == 0):
			nav_center09()
		elif (state == 2):
			if (trough < 10):
				state = 0
			elif (trough < 20):
				state = 3
			elif (trough < 30):
				state = 4
			elif (trough < 40):
				state = 5
			else:
				state = 6
			for i in range(len(pose_estimate)):
				move(0,0)
				if ((pose_estimate[i][1] < 0.77) and (pose_estimate[i][0] < 0.38) and (pose_estimate[i][0] != 0) and (pose_estimate[i][1] != 0)):
					move(0,0)
					#sleep(0.10)
					print (f"Obj{i} Identified at {markerID}")
					givedelay()
					ur5_pose_1 = geometry_msgs.msg.Pose()
					ur5_pose_1.position.x = pose_estimate[i][0]
					ur5_pose_1.position.y = pose_estimate[i][1] - 0.35 #waypoint for easyplanning
					ur5_pose_1.position.z = pose_estimate[i][2]
					ur5_pose_1.orientation.x = 0
					ur5_pose_1.orientation.y = 0
					ur5_pose_1.orientation.z = 0
					ur5_pose_1.orientation.w = math.radians(180)
					lst_joint_angles_1 = [math.radians(90),0.59,-0.32,0,0,0]
					all_zero = [0,0,0,0,0,0]
					ur5.set_joint_angles(lst_joint_angles_1)
					flag = 0
					count_attempt = 0
					while(count_attempt < 3):
						flag = ur5.go_to_pose(ur5_pose_1)
						if flag == True:
							break
						count_attempt = count_attempt + 1
					if (flag == True):
						ur5_pose_2 = geometry_msgs.msg.Pose()
						ur5_pose_2.position.x = pose_estimate[i][0]
						ur5_pose_2.position.y = pose_estimate[i][1] - 0.25 #offset between the gripper and wrist_3 link
						ur5_pose_2.position.z = pose_estimate[i][2]
						ur5_pose_2.orientation.x = 0
						ur5_pose_2.orientation.y = 0
						ur5_pose_2.orientation.z = 0
						ur5_pose_2.orientation.w = math.radians(180)
						ur5_pose_3 = geometry_msgs.msg.Pose()
						ur5_pose_3.position.x = pose_estimate[i][0]
						ur5_pose_3.position.y = pose_estimate[i][1] - 0.40 #waypoint for planning after tomato is gripped
						ur5_pose_3.position.z = pose_estimate[i][2]
						ur5_pose_3.orientation.x = 0
						ur5_pose_3.orientation.y = 0
						ur5_pose_3.orientation.z = 0
						ur5_pose_3.orientation.w = math.radians(180)
						flag = 0
						while flag == 0:
							flag = ur5.go_to_pose(ur5_pose_2)
						gripper.set_joint_angles(gripper_angle_close)
						print(f"Obj{i} Picked")
						flag = 0
						while(flag == 0):
							flag = ur5.go_to_pose(ur5_pose_1)
						ur5.set_joint_angles(all_zero)
						gripper.set_joint_angles(gripper_angle_open)
						print(f"Obj{i} Dropped in dropbox")
					focus_list = [1.57,-0.61,-1,1.585,0,0]
					ur5.set_joint_angles(focus_list)
			pose_estimate = [(0,0,0)]*len(pose_estimate)
		elif (state == 3):
			turn_center_to_left()
			nav_left90()	
		elif (state == 4):
			nav_left_to_right()
			turn_rightlane()
			nav_right1019()	
		elif (state == 5):
			turn_right_to_center()
			nav_center1910()
		elif (state == 6):
			finalpark()
			state = 7
			print("Mission Accomplished!")
			sleep (0.50)
			break
	

main_program()
